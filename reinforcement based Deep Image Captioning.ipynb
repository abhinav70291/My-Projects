{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning-based Image Captioning with Embedding Reward\n",
    "Abhinav Anurag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Working on:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from utils.image_utils import image_from_url\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Working on: \", device)\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "max_seq_len = 17\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MS-COCO data\n",
    "We will use the Microsoft COCO dataset for captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n",
      "train_captions_lens <class 'numpy.ndarray'> (400135,) float64\n",
      "val_captions_lens <class 'numpy.ndarray'> (195954,) float64\n"
     ]
    }
   ],
   "source": [
    "# Load COCO data from disk; this returns a dictionary\n",
    "# We'll work with dimensionality-reduced features for this notebook, but feel\n",
    "# free to experiment with the original features by changing the flag below.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "data[\"train_captions_lens\"] = np.zeros(data[\"train_captions\"].shape[0])\n",
    "data[\"val_captions_lens\"] = np.zeros(data[\"val_captions\"].shape[0])\n",
    "for i in range(data[\"train_captions\"].shape[0]):\n",
    "    data[\"train_captions_lens\"][i] = np.nonzero(data[\"train_captions\"][i] == 2)[0][0] + 1\n",
    "for i in range(data[\"val_captions\"].shape[0]):\n",
    "    data[\"val_captions_lens\"][i] = np.nonzero(data[\"val_captions\"][i] == 2)[0][0] + 1\n",
    "\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = load_coco_data(max_train=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        \n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        \n",
    "        self.cnn2linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim, batch_first=True)\n",
    "        self.linear2vocab = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        hidden_init = self.cnn2linear(features)\n",
    "        cell_init = torch.zeros_like(hidden_init)\n",
    "        output, _ = self.lstm(input_captions, (hidden_init, cell_init))\n",
    "        output = self.linear2vocab(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(RewardNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = torch.zeros(1, 1, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.gru = nn.GRU(wordvec_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.gru(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class RewardNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.rewrnn = RewardNetworkRNN(word_to_idx)\n",
    "        self.visual_embed = nn.Linear(512, 512)\n",
    "        self.semantic_embed = nn.Linear(512, 512)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            rrnn = self.rewrnn(captions[:, t])\n",
    "        rrnn = rrnn.squeeze(0).squeeze(1)\n",
    "        se = self.semantic_embed(rrnn)\n",
    "        ve = self.visual_embed(features)\n",
    "        return ve, se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super(ValueNetworkRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = (torch.zeros(1, 1, self.hidden_dim).to(device), torch.zeros(1, 1, self.hidden_dim).to(device))\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.lstm = nn.LSTM(wordvec_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.lstm(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.valrnn = ValueNetworkRNN(word_to_idx)\n",
    "        self.linear1 = nn.Linear(1024, 512)\n",
    "        self.linear2 = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        for t in range(captions.shape[1]):\n",
    "            vrnn = self.valrnn(captions[:, t])\n",
    "        vrnn = vrnn.squeeze(0).squeeze(1)\n",
    "        state = torch.cat((features, vrnn), dim=1)\n",
    "        output = self.linear1(state)\n",
    "        output = self.linear2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueNetwork(\n",
       "  (valrnn): ValueNetworkRNN(\n",
       "    (caption_embedding): Embedding(1004, 512)\n",
       "    (lstm): LSTM(512, 512)\n",
       "  )\n",
       "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "policyNet.train(mode=False)\n",
    "\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet.load_state_dict(torch.load('valueNetwork.pt'))\n",
    "valueNet.train(mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptions(features, captions, model):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    for t in range(max_seq_len-1):\n",
    "        output = model(features, gen_caps)\n",
    "        gen_caps = torch.cat((gen_caps, output[:, -1:, :].argmax(axis=2)), axis=1)\n",
    "    return gen_caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptionsWithBeamSearch(features, captions, model, beamSize=5):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    candidates = [(gen_caps, 0)]\n",
    "    for t in range(max_seq_len-1):\n",
    "        next_candidates = []\n",
    "        for c in range(len(candidates)):\n",
    "            output = model(features, candidates[c][0])\n",
    "            probs, words = torch.topk(output[:, -1:, :], beamSize)\n",
    "            for i in range(beamSize):\n",
    "                cap = torch.cat((candidates[c][0], words[:, :, i]), axis=1)\n",
    "                score = candidates[c][1] - torch.log(probs[0, 0, i]).item()\n",
    "                next_candidates.append((cap, score))\n",
    "        ordered_candidates = sorted(next_candidates, key=lambda tup:tup[1])\n",
    "        candidates = ordered_candidates[:beamSize]\n",
    "    return candidates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lookahead Inference with Policy and Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCaptionsWithBeamSearchValueScoring(features, captions, model, beamSize=5):\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    gen_caps = torch.tensor(captions[:, 0:1], device=device).long()\n",
    "    candidates = [(gen_caps, 0)]\n",
    "    for t in range(max_seq_len-1):\n",
    "        next_candidates = []\n",
    "        for c in range(len(candidates)):\n",
    "            output = model(features, candidates[c][0])\n",
    "            probs, words = torch.topk(output[:, -1:, :], beamSize)\n",
    "            for i in range(beamSize):\n",
    "                cap = torch.cat((candidates[c][0], words[:, :, i]), axis=1)\n",
    "                value = valueNet(features.squeeze(0), cap).detach()\n",
    "                score = candidates[c][1] - 0.6*value.item() -0.4*torch.log(probs[0, 0, i]).item()\n",
    "                next_candidates.append((cap, score))\n",
    "        ordered_candidates = sorted(next_candidates, key=lambda tup:tup[1])\n",
    "        candidates = ordered_candidates[:beamSize]\n",
    "    return candidates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    max_seq_len = 17\n",
    "    captions, features, urls = sample_coco_minibatch(small_data, batch_size=100, split='val')\n",
    "    for i in range(100):\n",
    "        gen_caps = []\n",
    "        gen_caps.append(GenerateCaptions(features[i:i+1], captions[i:i+1], policyNet)[0])\n",
    "        gen_caps.append(GenerateCaptionsWithBeamSearch(features[i:i+1], captions[i:i+1], policyNet)[0][0][0])\n",
    "        gen_caps.append(GenerateCaptionsWithBeamSearchValueScoring(features[i:i+1], captions[i:i+1], policyNet)[0][0][0])\n",
    "        decoded_tru_caps = decode_captions(captions[i], data[\"idx_to_word\"])\n",
    "\n",
    "#         f = open(\"truth3.txt\", \"a\")\n",
    "#         f.write(decoded_tru_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        \n",
    "#         decoded_gen_caps = decode_captions(gen_caps[0], data[\"idx_to_word\"])\n",
    "#         f = open(\"greedy3.txt\", \"a\")\n",
    "#         f.write(decoded_gen_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        \n",
    "#         decoded_gen_caps = decode_captions(gen_caps[1], data[\"idx_to_word\"])\n",
    "#         f = open(\"beam3.txt\", \"a\")\n",
    "#         f.write(decoded_gen_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        \n",
    "#         decoded_gen_caps = decode_captions(gen_caps[2], data[\"idx_to_word\"])\n",
    "#         f = open(\"policyvalue3.txt\", \"a\")\n",
    "#         f.write(decoded_gen_caps + \"\\n\")\n",
    "#         f.close()\n",
    "        try:\n",
    "            plt.imshow(image_from_url(urls[i]))\n",
    "            plt.show()\n",
    "        except:\n",
    "            continue\n",
    "        print(urls[i])\n",
    "        print(decode_captions(gen_caps[2], data[\"idx_to_word\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU_score(gt_caption, sample_caption, w):\n",
    "    \"\"\"\n",
    "    gt_caption: string, ground-truth caption\n",
    "    sample_caption: string, your model's predicted caption\n",
    "    Returns unigram BLEU score.\n",
    "    \"\"\"\n",
    "    reference = [x for x in gt_caption.split(' ') \n",
    "                 if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    hypothesis = [x for x in sample_caption.split(' ') \n",
    "                  if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [w])\n",
    "    return BLEUscore\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    model: CaptioningRNN model\n",
    "    Prints unigram BLEU score averaged over 1000 training and val examples.\n",
    "    \"\"\"\n",
    "    BLEUscores = {}\n",
    "    for split in ['train', 'val']:\n",
    "        minibatch = sample_coco_minibatch(data, split=split, batch_size=1000)\n",
    "        gt_captions, features, urls = minibatch\n",
    "        gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "        sample_captions = model.sample(features)\n",
    "        sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
    "\n",
    "        total_score = 0.0\n",
    "        for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
    "            total_score += BLEU_score(gt_caption, sample_caption)\n",
    "\n",
    "        BLEUscores[split] = total_score / len(sample_captions)\n",
    "\n",
    "    for split in BLEUscores:\n",
    "        print('Average BLEU score for %s: %f' % (split, BLEUscores[split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps0 = []\n",
    "caps1 = []\n",
    "caps2 = []\n",
    "caps3 = []\n",
    "f = open(\"truth3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps0.append(x)\n",
    "f = open(\"greedy3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps1.append(x)\n",
    "f = open(\"beam3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps2.append(x)\n",
    "f = open(\"policyvalue3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps3.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy BLEU-1 : 0.3374543171912208\n",
      "Beam BLEU-1 : 0.29998119046207933\n",
      "Agent BLEU-1 : 0.30057253835441977\n",
      "\n",
      "Greedy BLEU-2 : 0.18381039209700356\n",
      "Beam BLEU-2 : 0.13227059725207552\n",
      "Agent BLEU-2 : 0.1331405488671185\n",
      "\n",
      "Greedy BLEU-3 : 0.12767973218661097\n",
      "Beam BLEU-3 : 0.0724795981070803\n",
      "Agent BLEU-3 : 0.07345311185936992\n",
      "\n",
      "Greedy BLEU-4 : 0.10370808190929426\n",
      "Beam BLEU-4 : 0.04722721512979743\n",
      "Agent BLEU-4 : 0.04818825165316483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b1, b2, b3 = 0, 0, 0\n",
    "for w in range(1, 5):\n",
    "    for i in range(len(caps0)):\n",
    "        b1 += BLEU_score(caps0[i], caps1[i], w)\n",
    "        b2 += BLEU_score(caps0[i], caps2[i], w)\n",
    "        b3 += BLEU_score(caps0[i], caps3[i], w)\n",
    "    b1 /= len(caps0)\n",
    "    b2 /= len(caps0)\n",
    "    b3 /= len(caps0)\n",
    "    print(\"Greedy BLEU-\" + str(w), \":\", b1)\n",
    "    print(\"Beam BLEU-\" + str(w), \":\", b2)\n",
    "    print(\"Agent BLEU-\" + str(w), \":\", b3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of references is 1000\n",
      "{'testlen': 10382, 'reflen': 9365, 'guess': [10382, 9382, 8382, 7382], 'correct': [2832, 600, 168, 52]}\n",
      "ratio: 1.10859583555781\n",
      "{'Bleu_1': 0.2727798112116863, 'Bleu_2': 0.13207907633440683, 'Bleu_3': 0.0704492797131235, 'Bleu_4': 0.03961546368562404, 'METEOR': 0.1086774165301927, 'ROUGE_L': 0.2611785262449472, 'CIDEr': 0.4247729924235357}\n"
     ]
    }
   ],
   "source": [
    "ref, hypo = metrics.load_textfiles(caps0, caps3)\n",
    "print(metrics.score(ref, hypo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = False\n",
    "\n",
    "policyNetwork = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(policyNetwork.parameters(), lr=0.0001)\n",
    "\n",
    "if pretrained:\n",
    "    policyNetwork.load_state_dict(torch.load('models/policyNetwork.pt'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 256846 loss: 0.2919604778289795\n",
      "epoch: 271081 loss: 0.2918578088283539\n",
      "epoch: 276987 loss: 0.2405262291431427\n",
      "epoch: 324988 loss: 0.2366735190153122\n",
      "epoch: 339368 loss: 0.20999650657176971\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "bestLoss = 0.3\n",
    "#0.006700546946376562\n",
    "\n",
    "for epoch in range(250000, 350000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float().unsqueeze(0)\n",
    "    captions_in = torch.tensor(captions[:, :-1], device=device).long()\n",
    "    captions_ou = torch.tensor(captions[:, 1:], device=device).long()\n",
    "    output = policyNetwork(features, captions_in)\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        caplen = np.nonzero(captions[i] == 2)[0][0] + 1\n",
    "        loss += (caplen/batch_size)*criterion(output[i][:caplen], captions_ou[i][:caplen])\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(policyNetwork.state_dict(), \"policyNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Reward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNetwork = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "optimizer = optim.Adam(rewardNetwork.parameters(), lr=0.001)\n",
    "\n",
    "# https://cs230-stanford.github.io/pytorch-nlp.html#writing-a-custom-loss-function\n",
    "def VisualSemanticEmbeddingLoss(visuals, semantics):\n",
    "    beta = 0.2\n",
    "    N, D = visuals.shape\n",
    "    \n",
    "    visloss = torch.mm(visuals, semantics.t())\n",
    "    visloss = visloss - torch.diag(visloss).unsqueeze(1)\n",
    "    visloss = visloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    visloss = F.relu(visloss)\n",
    "    visloss = torch.sum(visloss)/N\n",
    "    \n",
    "    semloss = torch.mm(semantics, visuals.t())\n",
    "    semloss = semloss - torch.diag(semloss).unsqueeze(1)\n",
    "    semloss = semloss + (beta/N)*(torch.ones((N, N)).to(device) - torch.eye(N).to(device))\n",
    "    semloss = F.relu(semloss)\n",
    "    semloss = torch.sum(semloss)/N\n",
    "    \n",
    "    return visloss + semloss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "\n",
    "for epoch in range(50000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    captions = torch.tensor(captions, device=device).long()\n",
    "    ve, se = rewardNetwork(features, captions)\n",
    "    loss = VisualSemanticEmbeddingLoss(ve, se)\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(rewardNetwork.state_dict(), \"rewardNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    rewardNetwork.rewrnn.hidden_cell.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRewards(features, captions, model):\n",
    "    visEmbeds, semEmbeds = model(features, captions)\n",
    "    visEmbeds = F.normalize(visEmbeds, p=2, dim=1) \n",
    "    semEmbeds = F.normalize(semEmbeds, p=2, dim=1) \n",
    "    rewards = torch.sum(visEmbeds*semEmbeds, axis=1).unsqueeze(1)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RewardNetwork(\n",
      "  (rewrnn): RewardNetworkRNN(\n",
      "    (caption_embedding): Embedding(1004, 512)\n",
      "    (gru): GRU(512, 512)\n",
      "  )\n",
      "  (visual_embed): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (semantic_embed): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "PolicyNetwork(\n",
      "  (caption_embedding): Embedding(1004, 512)\n",
      "  (cnn2linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (lstm): LSTM(512, 512, batch_first=True)\n",
      "  (linear2vocab): Linear(in_features=512, out_features=1004, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ValueNetwork(\n",
       "  (valrnn): ValueNetworkRNN(\n",
       "    (caption_embedding): Embedding(1004, 512)\n",
       "    (lstm): LSTM(512, 512)\n",
       "  )\n",
       "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewardNet = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "rewardNet.load_state_dict(torch.load('rewardNetwork.pt'))\n",
    "for param in rewardNet.parameters():\n",
    "    param.require_grad = False\n",
    "print(rewardNet)\n",
    "\n",
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "for param in policyNet.parameters():\n",
    "    param.require_grad = False\n",
    "print(policyNet)\n",
    "\n",
    "valueNetwork = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(valueNetwork.parameters(), lr=0.0001)\n",
    "valueNetwork.train(mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "bestLoss = 10000\n",
    "max_seq_len = 17\n",
    "\n",
    "for epoch in range(50000):\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=batch_size, split='train')\n",
    "    features = torch.tensor(features, device=device).float()\n",
    "    \n",
    "    # Generate captions using the policy network\n",
    "    captions = GenerateCaptions(features, captions, policyNet)\n",
    "    \n",
    "    # Compute the reward of the generated caption using reward network\n",
    "    rewards = GetRewards(features, captions, rewardNet)\n",
    "    \n",
    "    # Compute the value of a random state in the generation process\n",
    "#     print(features.shape, captions[:, :random.randint(1, 17)].shape)\n",
    "    values = valueNetwork(features, captions[:, :random.randint(1, 17)])\n",
    "    \n",
    "    # Compute the loss for the value and the reward\n",
    "    loss = criterion(values, rewards)\n",
    "    \n",
    "    if loss.item() < bestLoss:\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(valueNetwork.state_dict(), \"valueNetwork.pt\")\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    valueNetwork.valrnn.hidden_cell[0].detach_()\n",
    "    valueNetwork.valrnn.hidden_cell[1].detach_()\n",
    "    rewardNet.rewrnn.hidden_cell.detach_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Advantage Actor Critic Model for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvantageActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, valueNet, policyNet):\n",
    "        super(AdvantageActorCriticNetwork, self).__init__()\n",
    "\n",
    "        self.valueNet = valueNet #RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "        self.policyNet = policyNet #PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # Get value from value network\n",
    "        values = self.valueNet(features, captions)\n",
    "        # Get action probabilities from policy network\n",
    "        probs = self.policyNet(features.unsqueeze(0), captions)[:, -1:, :]        \n",
    "        return values, probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardNet = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "rewardNet.load_state_dict(torch.load('rewardNetwork.pt'))\n",
    "policyNet.load_state_dict(torch.load('policyNetwork.pt'))\n",
    "valueNet.load_state_dict(torch.load('valueNetwork.pt'))\n",
    "\n",
    "a2cNetwork = AdvantageActorCriticNetwork(valueNet, policyNet)\n",
    "optimizer = optim.Adam(a2cNetwork.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculum = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "episodes = 50\n",
    "\n",
    "small_data = load_coco_data(max_train=50000)\n",
    "\n",
    "for level in curriculum:\n",
    "    \n",
    "    for epoch in range(1000):        \n",
    "        episodicAvgLoss = 0\n",
    "        \n",
    "        captions, features, _ = sample_coco_minibatch(small_data, batch_size=episodes, split='train')\n",
    "        features = torch.tensor(features, device=device).float()\n",
    "        captions = torch.tensor(captions, device=device).long()\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            caplen = np.nonzero(captions[episode] == 2)[0][0] + 1\n",
    "            \n",
    "            if (caplen - level > 1):\n",
    "                captions_in = captions[episode:episode+1, :caplen-level]\n",
    "                features_in = features[episode:episode+1]\n",
    "\n",
    "                for step in range(level):\n",
    "                    value, probs = a2cNetwork(features_in, captions_in)\n",
    "                    probs = F.softmax(probs, dim=2)\n",
    "                    \n",
    "                    dist = probs.cpu().detach().numpy()[0,0]\n",
    "                    action = np.random.choice(probs.shape[-1], p=dist)\n",
    "                    \n",
    "                    gen_cap = torch.from_numpy(np.array([action])).unsqueeze(0).to(device)\n",
    "                    captions_in = torch.cat((captions_in, gen_cap), axis=1)\n",
    "                    \n",
    "                    log_prob = torch.log(probs[0, 0, action])\n",
    "                    \n",
    "                    reward = GetRewards(features_in, captions_in, rewardNet)\n",
    "                    reward = reward.cpu().detach().numpy()[0, 0]\n",
    "                    \n",
    "                    rewards.append(reward)\n",
    "                    values.append(value)\n",
    "                    log_probs.append(log_prob)\n",
    "                    \n",
    "            values = torch.FloatTensor(values).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).to(device)\n",
    "            log_probs = torch.stack(log_probs).to(device)\n",
    "            \n",
    "            advantage = values - rewards \n",
    "            actorLoss = (-log_probs * advantage).mean()\n",
    "            criticLoss = 0.5 * advantage.pow(2).mean()\n",
    "            \n",
    "            loss = actorLoss + criticLoss\n",
    "            episodicAvgLoss += loss.item()/episodes\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(epoch, \":\", episodicAvgLoss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
